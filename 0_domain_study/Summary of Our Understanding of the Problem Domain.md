# Summary of Our Understanding of the Problem Domain

## How We Used Divergent and Convergent Thinking?

We started with divergent thinking, where each team member shared several
problem areas related to technology, health, and personal experiences. After
presenting these ideas in a group session, we used a convergent phase to
evaluate each proposal using a matrix of criteria (relevance, feasibility,
data availability, etc.)

From this structured approach, we selected three top-scoring problems. After
further discussing constraints (data, timeline, expertise), we narrowed it
down to this problem as the most well-scoped and impactful within our limits.

## Systems Thinking – Iceberg Model Analysis

### Event - React: What just happened?

A user interacts with a mental health app and receives support from a chatbot.
They feel unheard, frustrated, or comforted—depending on the quality of the
response. In some cases, users report that chatbot replies were unhelpful,
generic, or even harmful during a crisis. In others, they report fast,
nonjudgmental support that helped de-escalate anxiety.

### Pattern/Trend - Anticipate: What has been happening over time?

There has been a rapid increase in mental health app usage, especially after
COVID-19 and amid growing global awareness of mental health.

Many apps have shifted from offering human-led services to chatbot-based or
hybrid models to reduce costs and scale access.

Mixed feedback has emerged: some users appreciate chatbots for privacy and
availability; others mistrust them due to perceived emotional insensitivity or
inability to handle complex issues.

Human support is often paywalled or limited to premium plans, raising equity
concerns.

### Structure - Design: What rules, policies, or institutions contribute to the event?

App developers optimize for scalability and cost-efficiency, often
prioritizing chatbot development over hiring human professionals.

Lack of universal guidelines or regulatory standards for emotional AI or
chatbot safety in healthcare-related apps.

Data privacy policies may limit what chatbots can store or respond to,
reducing conversational depth.

Mental health care systems in many countries are underfunded, pushing people
toward digital self-help tools by necessity.

### Mental Model - Transform: What values and beliefs keep the system in place?

The belief that "something is better than nothing" in mental health support
justifies reliance on AI chatbots.

The idea that digital tools can solve deeply human and complex problems like
mental health.

Societal stigma around mental health pushes people to seek anonymous,
non-human support—even if it's less effective.

There's a growing belief in AI as neutral or emotionally safe, despite
limitations in real empathy.
