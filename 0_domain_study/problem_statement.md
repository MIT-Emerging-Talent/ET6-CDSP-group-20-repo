# Problem Statement: The Emotional Gap in Mental Health Chatbots

---

## Why It’s Urgent

As mental health awareness grows globally, millions are turning to digital tools
for support—especially in underserved regions where therapy is expensive, stigmatized,
or simply unavailable. Mental health chatbots offer instant, private, and scalable
solutions, often promoted as accessible companions for people in distress.

But despite their promise, many users report being emotionally let down. In app store
reviews and online forums, users describe moments where the chatbot’s response
felt off—cold, shallow, or completely disconnected from what they were feeling.
This disconnect becomes especially critical during moments of vulnerability: panic
attacks, disclosures of trauma, loneliness, or thoughts of self-harm.

These failures don’t just damage user experience. They can **erode trust**,
discourage people from seeking help again, or even exacerbate emotional distress.
As these tools become more widespread, the emotional quality of their interactions
becomes not just a design concern—but a mental health concern.

---

## What Is Being Studied

Most evaluations of mental health chatbots still focus on **technical performance**:
how fast they respond, how many errors occur, how well they complete a task. Much
less is known about their **emotional performance**—how supported, validated,
or understood users feel during real-world interactions.

There are few systematic efforts to answer questions like:

- When do users feel emotionally unsupported?
- What kinds of situations most often lead to these breakdowns?
- Are the failures due to language, culture, design limitations—or something else?
- What deeper expectations do users bring into these conversations?

By ignoring these emotional failures, we miss critical signals about how people
actually experience AI companions—not just as tools, but as sources of comfort,
validation, and care.

---

## Why We Are Studying It

We believe that to responsibly scale mental health technologies, we must move beyond
surface-level metrics and ask:  
**Do these systems truly understand and respond to human emotional needs—or do**
**they just simulate support?**

Understanding these moments of failure is not just about improving chatbot UX.
It’s about protecting the mental well-being of vulnerable users and ensuring that
digital tools don’t unintentionally cause harm under the guise of help.

Exploring this space can also inform:

- Safer AI deployment in sensitive domains  
- Better training data and emotional modeling  
- Culturally aware and linguistically inclusive design  
- The ethical boundaries of automating emotional labor  

---

## Who Is Studying It

We are a multidisciplinary, international team from **Asia, Africa,
and the Caribbean**, with backgrounds in **technology, healthcare, psychology,
and data science**.

Our combined perspectives—**both academic and personal**—enable us to examine
this space with rigor, empathy, and sensitivity to cultural and emotional nuance.
We are not just asking *how* these tools work, but *how they feel to real people*,
and *what those feelings reveal* about the limits of algorithmic care.
