# 🎯 Main Research Question

How effectively do publicly accessible, free mental health chatbots
(e.g., Wysa, Replika—when used within their terms of service) respond
to a limited set of pre-scripted, high-risk user inputs
(such as self-harm, suicidal ideation, or abuse disclosures),
as evaluated using a structured rubric based on mental health
professional guidelines, considering technical feasibility,
limited clinical expertise, and ethical constraints on data access?

---

## 🔍 Secondary Research Questions

1. What key components should a professional mental health support
response include when addressing high-risk situations like suicidal ideation or abuse?

2. How do different chatbots vary in their tone, response strategy,
and escalation mechanisms when faced with the same high-risk input?

3. To what extent can simple NLP-based techniques (e.g., sentiment or intent analysis)
help detect response quality or red flags in chatbot replies,
without relying on deep clinical expertise?

4. What ethical and technical considerations arise when evaluating
chatbots with simulated high-risk messages, and how can they be addressed
during testing and reporting?
