# Advice and Best Practices

## For users (people using mental-health chatbots or other digital tools)

- **Treat chatbots as assistants, not therapists.** Use them for psychoeducation,
  self-help tools, reminders, or signposting — not for diagnoses, medication  
  changes, or severe symptoms (including suicidal thoughts). [1,2,7]

- **Confirm privacy — don’t overshare.** Before entering sensitive details check
  how the app stores, protects, and especially uses your data (training,  
  third-party sharing). If unclear, assume risk. [1,2,6]

- **Prefer tools with explicit crisis plans and human backup.** Use services that
  clearly describe how they detect risk and connect you to real help (tested  
  escalation/handover). [1,2,7]

- **Stop & report if the bot is biased, offensive, or culturally off.** If it  
  stereotypes, misunderstands, or gives inappropriate advice, discontinue use  
  and report the behavior. [2,3,4]

- **Verify medical or medication suggestions with a clinician.** Do not act on  
  diagnoses or medication advice without professional confirmation. [1,2,6]

---

## For product managers & developers  

(designing mental-health chatbots / DHIs)

- **State scope and limits clearly at onboarding.** Disclose what the tool does,
  who it is for, and what it is not (no vague therapeutic claims without  
  trials). [1,2,6]

- **Make crisis detection + escalation mandatory.** Implement suicide/risk  
  screening, automatic flagging, and reliable human handover pathways — and  
  test failure modes. [1,2,7]

- **Adopt privacy-by-design and give users control.** Minimize PII, allow  
  deletion/export, and require explicit consent for training or third-party  
  sharing. [1,2,6]

- **Continuously audit and mitigate bias.** Test performance across language,  
  age, gender, and socioeconomic groups; publish audits and mitigation steps. [1,2,3]

- **Co-design with lived-experience communities.** Run compensated PPI/co-design
  with diverse users — especially marginalized groups — and document outcomes. [1,4]

- **Be conservative with claims; require clinical validation.** Do not market as
  “therapy” or claim effectiveness until demonstrated in peer-reviewed trials. [1,2,6]

- **Set governance, liability, and accreditation plans.** Establish ethics  
  boards, incident reporting, and prepare for accreditation and liability  
  frameworks (TEQUILA: Liability, Accreditation). [1,5,6]

---

## Key terms

- **DHIs** → Digital Health Interventions  
- **PII** → Personally Identifiable Information  
- **PPI** → Patient and Public Involvement  

---

## TEQUILA framework (Löchner et al., 2025)

- **T – Trust** → Build confidence via transparency, evidence, ethics.  
- **E – Evidence** → Base claims on solid clinical research (RCTs preferred).  
- **Q – Quality** → Ensure technical robustness, accuracy, performance.  
- **U – Usability** → Make tools accessible, engaging, and inclusive.  
- **I – Interest** → Align with user needs, preferences, and cultural context.  
- **L – Liability** → Clarify accountability if harm occurs.  
- **A – Accreditation** → Seek certifications or approvals validating safety.  

In the article, TEQUILA is both:  

- A checklist for managers & developers during design/evaluation.  
- A communication tool for clinicians, regulators, and users.

---

## References

1. Löchner J., Carlbring P., Schuller B., Torous J., Sander L.B. *Digital  
   interventions in mental health: An overview and future perspectives.*  
   Internet Interventions (2025).  
   <https://www.sciencedirect.com/science/article/pii/S2214782925000259>

2. Xi Wang et al. *The Application and Ethical Implication of Generative AI in  
   Mental Health: Systematic Review.* JMIR Mental Health (2025).  
   <https://pubmed.ncbi.nlm.nih.gov/40577783/>

3. *A Call to Action on Assessing and Mitigating Bias in AI Applications for  
   Mental Health.* PMC.  
   <https://pmc.ncbi.nlm.nih.gov/articles/PMC10250563/>

4. *Ensuring patient and public involvement in the transition to AI-assisted  
   mental health care: Scoping review and agenda for design justice.* PMC.  
   <https://pmc.ncbi.nlm.nih.gov/articles/PMC8369091/>

5. *Regulating AI in Mental Health: Ethics of Care Perspective.* PMC.  
   <https://pmc.ncbi.nlm.nih.gov/articles/PMC11450345/>

6. *Ethical Considerations in AI Interventions for Mental Health and  
   Well-Being.* MDPI, Societies.  
   <https://www.mdpi.com/2076-0760/13/7/381>

7. *Exploring the Ethical Challenges of Conversational AI in Mental Health  
   Care: Scoping Review.* JMIR Mental Health (2025).  
   <https://mental.jmir.org/2025/1/e60432>
