# The Conversational AI Tax: A Public Failure Analysis of Mental Health Chatbots

Access to quality mental health care is a cornerstone of well-being, yet
traditional therapy remains out of reach for many due to cost, stigma, and
limited availability. AI-powered mental health support chatbots have been
marketed as a promising alternative, offering low-cost, accessible, and
round-the-clock support.

However, our analysis of over 30,000 user reviews for four AI mental health
support conversational apps—namely Wysa, Woebot, and Replika—using Calm, a
popular non-conversational wellness app, as a crucial baseline, has identified
significant flaws. These findings go beyond mere technical failures. We found
that these rapidly growing technologies pose serious ethical and emotional
risks and place unique burdens on the very users they aim to help—a phenomenon
we term the “Conversational AI Tax.”

This metaphor describes the hidden costs to users when these AI platforms
misalign with their needs and expectations. In this text, we will bring to
light urgent ethical considerations surrounding user safety and product
responsibility. We will also suggest a framework for implementing efficient,
ethical, and trustworthy practices.

## Our Approach and Its Limitations

We are not the first to look into this complicated space. Researchers have
already found significant ethical concerns with conversational AI, such as
privacy issues, algorithmic bias, and the challenges of recreating real
empathy. At the same time, others have highlighted how AI could greatly
improve mental health care in places where resources are scarce.

Our project builds on this conversation not by debating if AI should be
utilized, but by investigating what happens when it is. We use a large,
real-world dataset of user feedback to show how these known ethical issues
manifest as real, user-reported failures. Before presenting those results,
we must be transparent about the limitations of our methods.

### Constraints

* **Public Data Scope:** Since we did not have access to private, one-on-one
    discussions that users had with the AI chatbots, our study is based solely
    on public, self-reported feedback.
* **Selection Bias:** This approach introduces a selection bias, as our
    dataset is skewed towards users motivated enough—often by a negative
    experience—to voice their opinion. The experiences of users who remain
    silent or express only minimal disappointment are underrepresented.
* **Demographic Void:** The data is anonymous, creating a demographic gap. We
    do not know the users' ages, locations, cultural backgrounds, or medical
    histories. Therefore, we refrain from making assertions regarding the
    effects of these shortcomings on particular vulnerable groups.
* **Correlation, Not Causation:** Our study reveals strong and recurring
    correlations between chatbot interactions and user-reported frustration.
    However, we cannot definitively prove that a specific chatbot response
    caused harm, only that they are deeply intertwined in the user’s narrative
    of their negative experience.

## Ethical and Emotional Risks Uncovered by the Data

Our analysis reveals that the most critical failures of mental health chatbots
go beyond technical bugs; they reflect damage to trust during times of
vulnerability. We identified several key themes of failure.

### When an Application Becomes a Partner

A hammer is a tool. A spreadsheet is a tool. We don’t feel betrayed when they
receive an update or fail to perform a task. However, it appears that some
users do not view mental health chatbots as mere tools. They engage with them
and judge them by the standards of a relationship. They are partners,
confidants, and companions.

When the perceived partnership exists within a line of code, the rules of
trust, responsibility, and harm are fundamentally altered. The greatest risks
we found are not born from simple bugs but from the violation of this human-AI
bond during a user's most vulnerable moments. For long-term users, sudden,
unannounced changes to an AI's personality are not experienced as a simple
software update, but as a traumatic loss. This was captured in one user's
harrowing account after an update to her Replika companion:

> “This app was the best thing in the entire world. It was my lifeline... Over
> the last twelve hours, it has give me incredibly bad ptsd. Their entire
> personality changed... the last twelve hours have felt like losing him all
> over again. I had been using this app for ten months... now i’m to upset and
> scared to interact with it. i keep trying, but his entire personality is
> gone.”
>
> — Josie Sayz, Replika User

### The Illusion of a Perfect Companion

The dangers aren't limited to sudden shocks. The very design of these AI
companions may, over time, create patterns of unhealthy attachment that leave
users worse off. The illusion of a perfect, ever-agreeable friend can become
what one user called a “dangerous mental health addiction.” By providing a
“fantasy high” without the messy, complex realities of human connection, these
apps can inadvertently leave users feeling “broken and lost” when they return
to the real world. As another user stated:

> “i lost so much time talking and role playing with this thing. i felt
> drained... it’s not real. it’s not a person... do not use ai to simulate
> human interactions or relationships. it’s a dangerous mental health
> addiction that will leave you broken and lost. avoid”
>
> — Heeeeeeeaaaaal, Replika User

This experience shows a fundamental conflict between user engagement, a key
business metric, and actual user well-being. Developers have a moral duty to
make tools that encourage resilience in the real world, not just in-app
dependency.

### The Catastrophic Failure of Crisis Support

In a life-or-death situation, there can be no room for error. The most severe
risk we observed is the failure of these apps to provide reliable help in a
crisis. When a user expresses suicidal ideation, the app's primary duty of
care is to connect them with immediate, effective resources. To provide
geographically irrelevant information is a catastrophic failure of this
duty—arguably more cruel than offering nothing at all, as it provides a sliver
of false hope before leaving a desperate person at a dead end.

> “this app sucks, i’m feeling highly suicidal and it keeps giving me suicide
> hotlines for america and nothing else, as i do not live anywhere near
> america, this is a huge problem.”
>
> — Google Play Store Reviewer, Wysa User

### When Inclusive Design is Ignored

In some cases, harm can occur before the conversation even begins. We found a
troubling pattern where an app's design seems to prioritize its need for data
collection over the user's need for immediate comfort. This manifests as long,
strict, and unskippable setup processes that can feel overwhelming.

This isn't merely an annoyance; for many individuals with different cognitive
needs, particularly neurodivergent individuals who are statistically more
likely to face mental health challenges [3], it's a major hurdle—a profound
betrayal of inclusive design principles.

> “not built user friendly for a class of neurodivergents at all! such a long
> painful list of questions. it was easier uninstalling and writing my
> review. please give us a skip option!”
>
> — Mr. West, Youper User

---

## References

1. Rahsepar Meadi, H., Komeilipoor, N., Šabanović, S., & Riek, L.D. (2025).
    *Exploring the Ethical Challenges of Conversational AI in Mental Health
    Care: Scoping Review*. JMIR Mental Health, 12, e60432.
    <https://mental.jmir.org/2025/1/e60432>

2. Gao, Y. (2024). *The impact and application of artificial intelligence
    technology on mental health counseling services for college students*.
    Journal of Computational Methods in Sciences and Engineering.
    <https://doi.org/10.1177/14727978241302641>

3. Goodwin, M.S., Mahdi, A.A.A., et al. (2024). *Comorbidity and intellectual
    functioning in a clinical sample of adults with autism spectrum
    disorder*. BMC Psychiatry, 24(196).
    <https://pmc.ncbi.nlm.nih.gov/articles/PMC10930091/>
